<!DOCTYPE HTML>
<html>

<head>
	<link rel="bookmark"  type="image/x-icon"  href="/img/knight.jpg"/>
	<link rel="shortcut icon" href="/img/knight.jpg">
	
			    <title>
    CLKO
    </title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="/css/mic_main.css" />
    <link rel="stylesheet" href="/css/dropdownMenu.css" />
    <meta name="keywords" content="clko" />
    
    <noscript>
        <link rel="stylesheet" href="/css/noscript.css" />
    </noscript>
    <style type="text/css">
        body:before {
          content: ' ';
          position: fixed;
          top: 0;
          background: url('/img/bg.png') center 0 no-repeat;
          right: 0;
          bottom: 0;
          left: 0;
          background-size: cover; 
        }
    </style>

			    
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script async type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


    <script src="/js/jquery.min.js"></script>
    <script src="/js/jquery.scrollex.min.js"></script>
    <script src="/js/jquery.scrolly.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/util.js"></script>
    <script src="/js/main.js"></script>
	
<meta name="generator" content="Hexo 5.3.0"></head>
    
		
<!-- Layouts -->



<!--  代码渲染  -->
<link rel="stylesheet" href="/css/prism_coy.css" />
<link rel="stylesheet" href="/css/typo.css" />

<!-- 文章页 -->
<body class="is-loading">
    <!-- Wrapper 外包 s-->
    <div id="wrapper" class="fade-in">
        <!-- Intro 头部显示 s -->
        <!-- Intro 头部显示 e -->
        <!-- Header 头部logo start -->
        <header id="header">
    <a href="/" class="logo">CLKO</a>
</header>
        <!-- Nav 导航条 start -->
        <nav id="nav" class="special" >
            <ul class="menu links" >
			<!-- Homepage  主页  --> 
			<li >
	            <a href="/" rel="nofollow">CLKO</a>
	        </li>
			<!-- categories_name  分类   --> 
	        
	        <li class="active">
	            <a href="#s1">Categories</a>
	                    <ul class="submenu">
	                        <li>
	                        <a class="category-link" href="/categories/Problem-solving/">Problem solving</a>
	                    </ul>
	        </li>
	        
	        <!-- archives  归档   --> 
	        
	        
		        <!-- Pages 自定义   -->
		        
		        <li>
		            <a href="/group/" title="Others">
		                Others
		            </a>
		        </li>
		        
		        <li>
		            <a href="/tag/" title="Tags">
		                Tags
		            </a>
		        </li>
		        


            </ul>
            <!-- icons 图标   -->
			<ul class="icons">
                    
                    <li>
                        <a title="github" href="https://github.com/CLKO-health" target="_blank" rel="noopener">
                            <i class="icon fa fa-github"></i>
                        </a>
                    </li>
                    
                    <li>
                        <a title="linkedin" href="https://www.linkedin.com/in/chu-ling-ko/" target="_blank" rel="noopener">
                            <i class="icon fa fa-linkedin"></i>
                        </a>
                    </li>
                    
			</ul>
</nav>

        <div id="main" >
            <div class ="post_page_title_img" style="height: 25rem;background-image: url(/images/hashcode.jpg);background-position: center; background-repeat:no-repeat; background-size:cover;-moz-background-size:cover;overflow:hidden;" >
                <a href="#" style="padding: 4rem 4rem 2rem 4rem ;"><h2 >Hash Code For Intern 2020</h2></a>
            </div>
            <!-- Post -->
            <div class="typo" style="padding: 3rem;">
                <h1 id="Hash-Code-For-Intern-2020-My-Experience-Sharing"><a href="#Hash-Code-For-Intern-2020-My-Experience-Sharing" class="headerlink" title="Hash Code For Intern 2020, My Experience Sharing"></a>Hash Code For Intern 2020, My Experience Sharing</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>When I was a summer intern in Google, their Hash Code Team hosted a competition named “Hash code for intern” for our interns to participate. They said that the problem descriptions would be the same as one of the past years’ hash code competition’s problem, but the dataset would be different.</p>
<p>I think Hash code competition is much different from Google Code jam. It is more similar to <a target="_blank" rel="noopener" href="http://iccad-contest.org/2020/index.html"><strong><font color='blue'>CAD Contest at ICCAD</font></strong></a>. The problems are NP-hard problems. Although the problem format is similar to the normal algorithm problems, the exponential-time exhaustive search is required if we want to find the “optimal solution”. Therefore, this kind of competition usually goes with a scoring function to evaluate the goodness of the solution, which is also used to decide the ranking. This is kinda like the machine learning competitions that use accuracy to decide the ranking. And by the way, I think those machine learning algorithms aren’t recommended for solving hash code or CAD contest problems.</p>
<p>The datasets of Hash Code competition would be entirely released to the participants. The final score is calculated by summing the scores of the 5 datasets, and those 5 datasets have different characteristics. Participants can design highly targeted algorithms for each input to get high scores. Or I should say, Hash Code competition very encourages participants to analyze the characteristics of each input and design the specific algorithms. And because of this, even though “Hash Code For Intern”‘s problem would use the old problem, as long as the testcases are regenerated with five different properties, it can still avoid people who have seen the questions/solutions before having a too big advantage. (And before the competition, we didn’t know which year’s problem would be used.)</p>
<p>However, the difference between “Hash code for interns” and “Hash code” is that “Hash code” has a time limit of 4 hours, while “Hash code for intern” gave us 24 hours to solve the problems. I think it’s probably because Google interns are from all over the world(for example, Europe), and the time is decided by considering fairness. I think that in this case, “Hash code” should also be 24 hours, otherwise the contest holding time in Taiwan is 2 to 6 in the morning, which is too unfavorable for people living in Taiwan.</p>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>It seemed that not too many people participated in this competition. After removing the teams that have only registered but not submitted any answers, there are only 159 teams left. Although this was a team-based game, I couldn’t find any teammates at all, but luckily, I participated in it sololy. <font color='red'>I was in third place!</font> I was really lucky. It was also because I really like this NP optimization problem, so I was very enthusiastic during the whole competition.</p>
<p>Now I am going to introduce the problems and my solution. Please read them if you are interested :)</p>
<h2 id="Problem-description"><a href="#Problem-description" class="headerlink" title="Problem description"></a>Problem description</h2><p>The problem of Hash Code For Intern is the same as the problem of <a target="_blank" rel="noopener" href="https://storage.googleapis.com/coding-competitions.appspot.com/HC/2017/hashcode2017_qualification_task.pdf"><strong><font color='blue'> 2017 Hash Code Qualification round </font></strong></a>, but unfortunately, I don’t know about the characteristics of the testcases of that year and this contest because I only have time to produce a general solution for all testcases, but didn’t have extra time to do research and optimization for each testcase. Data analysis is really not easy! QAQ</p>
<p><img src="https://i.imgur.com/58fAZ5R.png"></p>
<h3 id="Data-Center"><a href="#Data-Center" class="headerlink" title="Data Center"></a>Data Center</h3><p>We have a data center with all videos inside (at most $10^4$ videos). Each video has a different size.</p>
<h3 id="Cache-Servers"><a href="#Cache-Servers" class="headerlink" title="Cache Servers"></a>Cache Servers</h3><p>We have a lot of cache servers (at most $10^3$ servers). They are all empty at the begining, and has a fixed capacity.</p>
<h3 id="Endpoints"><a href="#Endpoints" class="headerlink" title="Endpoints"></a>Endpoints</h3><p>We have a lot of endpoints (at most $10^3$ endpoints). The endpoints can get the videos from the data center. The endpoint can also get the videos from any cache server if it’s connected to it.</p>
<ul>
<li>The latency for each endpoint to get the video from the data center is different</li>
<li>Each endpoint is connected to some cache servers, and the latency for each endpoint to get the video from each cache server is different (but is definitely faster than from the data center!)</li>
</ul>
<h3 id="Requests"><a href="#Requests" class="headerlink" title="Requests"></a>Requests</h3><p>We have a lot of requests (at most $10^6$ requests). Each request is described as “which endpoint” wants to get “which video” with “how many times”.</p>
<h3 id="What-we-can-decide"><a href="#What-we-can-decide" class="headerlink" title="What we can decide"></a>What we can decide</h3><p>We need to decide: For each cache server, which videos should be put in it?</p>
<p>Each video can be repeatedly put into different cache servers, but the total video sizes in each cache server cannot exceed the capacity limit.</p>
<p>As long as we put a video in a cache server, those endpoints which are connected to this cache server can now get this video from this cache server. Therefore, if the latency from an endpoint to a cache server is short, this operation can considerably reduce the time required for this endpoint to get this video.</p>
<p>Our goal is to, based on all the requests, try to minimize the time required for the endpoint to get the video in the requests.</p>
<h3 id="Scoring"><a href="#Scoring" class="headerlink" title="Scoring"></a>Scoring</h3><p>Once we’ve decided which videos to put in each cache server, we can calculate how much time we saved in total.</p>
<p>For each request (endpoint, video, times), originally the time needed is the latency from the data center to this endpoint, multiplied by the times. We call it $T$.</p>
<p>Assume that in our solution, some cache servers have this video in them. Then now the time this request needs is the sortest latency among all the cache servers which are connected to this endpoint and have this video in it, multiplied by the times. We call it $t$.</p>
<p>Therefore, in this request, the time we saved is $(T-t)$. The sum of the saved time of each request is the score of the solution!</p>
<h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h3><p>Let’s see an example.</p>
<p><img src="https://i.imgur.com/HcasXEX.png"></p>
<ul>
<li>Endpoint 0 wants:<ul>
<li>video 3, 1500 times</li>
<li>video 4, 500 times</li>
<li>video 1, 1000 times</li>
</ul>
</li>
<li>Endpoint 1 wants:<ul>
<li>video 0, 1000 times</li>
</ul>
</li>
</ul>
<p>Let’s use a table to calculate, how much time is used if we get all the videos from the data center without the cache servers:</p>
<table>
<thead>
<tr>
<th>Endpoint</th>
<th>Video</th>
<th>The latency from the data center</th>
<th>Times</th>
<th>Total used time</th>
</tr>
</thead>
<tbody><tr>
<td>0</td>
<td>3</td>
<td>1000</td>
<td>1500</td>
<td>1,500,000</td>
</tr>
<tr>
<td>0</td>
<td>4</td>
<td>1000</td>
<td>500</td>
<td>500,000</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>1000</td>
<td>1000</td>
<td>1,000,000</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>500</td>
<td>1000</td>
<td>500,000</td>
</tr>
</tbody></table>
<p>Therefore the total time required is $3,500,000$</p>
<p><img src="https://i.imgur.com/lXp7maC.png"></p>
<p>Then, this graph is a sample solution. It means:</p>
<ul>
<li>cache server 0 has：video 2</li>
<li>cache server 1 has：video 1 and video 3</li>
<li>cache server 2 has：video 0 and video 1</li>
</ul>
<p>Let’s use a table to calculate, how much time is used if we get the video from the source with the least latency:</p>
<table>
<thead>
<tr>
<th>Endpoint</th>
<th>Video</th>
<th>Selected video source</th>
<th>Latency</th>
<th>Times</th>
<th>Total used time</th>
</tr>
</thead>
<tbody><tr>
<td>0</td>
<td>3</td>
<td>cache server 1</td>
<td>300</td>
<td>1500</td>
<td>450,000</td>
</tr>
<tr>
<td>0</td>
<td>4</td>
<td>data center</td>
<td>1000</td>
<td>500</td>
<td>500,000</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>cache server 2</td>
<td>200</td>
<td>1000</td>
<td>200,000</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>data center</td>
<td>1000</td>
<td>1000</td>
<td>500,000</td>
</tr>
</tbody></table>
<p>Therefore the total time required is $1,650,000$</p>
<p>Therefore, the score of this solution on this dataset is $3,500,000-1,650,000=1,850,000$.</p>
<h2 id="My-Solution"><a href="#My-Solution" class="headerlink" title="My Solution"></a>My Solution</h2><p>In my opinion, I think there are two important points to deal with this NP optimization problem：<br>1. Maintain a delicate data structure so that the operations and computations can have the time complexity as low as possible.<br>2. Design a GREEDY strategy base on the scoring function</p>
<p>For simplicity, I defined some notations:</p>
<ul>
<li><p>$V_i$ means video $i$，$C_j$ means cache server $j$</p>
</li>
<li><p>$V_iC_j$ means the action “Put video $i$ into cache server $j$”. For example, “Put video $2$ into cache server $0$” is written as $V_2C_0$</p>
</li>
<li><p>$S=[V_{i_1}C_{j_1}, V_{i_2}C_{j_2}, …,V_{i_n}C_{j_n}]$ A set of actions is a solution.<br><img src="https://i.imgur.com/1t4L9H9.png"><br>For example, the action set in this graph’s solution is $[V_2C_0, V_1C_1, V_3C_1, V_0C_2, V_1C_2]$. The order can be arbitrary.</p>
</li>
<li><p>$score(S)$ means the score of the solution $S$. If $S$ is illegal (like some cache server is overloaded), we define $score(S)=-\infty$<br>Note that the value of $score(S)$ is irrevelant to the order of the actions in $S$.</p>
</li>
</ul>
<h3 id="Immediate-Reward"><a href="#Immediate-Reward" class="headerlink" title="Immediate Reward"></a>Immediate Reward</h3><p>My greedy strategy is to iteratively calculate how good each action “put some video into some cache server” is, and choose the best among all actions. The “goodness” is defined as “how many scores it can gain”, so I called this strategy “Immediate Reward”.</p>
<p>Let’s use the symbols to represent this. We use $S_t$ to represent the set of the actions which are already selected, then the immediate reward of the new action $V_iC_j$ is calculated as</p>
<p>$R(S_t, V_iC_j)=score(S_t\cup V_iC_j)-score(S_t)$</p>
<p>Note that the benefit of an action $V_iC_j$ is dependent on the already selected actions. For example:</p>
<ul>
<li>Putting $V_0$ into only $C_0$ gains 50 points</li>
<li>Putting $V_0$ into only $C_1$ gains 80 points</li>
<li>Putting $V_0$ into both $C_0$ and $C_1$ gains 100 points.</li>
</ul>
<p>Then if $V_0$ is not in any cache server yet, the reward of action $V_0C_0$ is $50$ (for example, there are a lot of endpoints that want to get $V_0$ from $C_0$). But if $V_0$ is already in $C_1$, then the reward of action $V_0C_0$ is only $20$ (for example, there are already many endpoints benefit from getting $V_0$ from $C_1$, so there would only be some endpoints benefit from getting $V_0$ from $C_0$)</p>
<p>As you can see, the calculation of the benefits should base on the currently selected solution $S$. And my greedy strategy is to iteratively select $V_iC_i$ with the maximum $R(S_t, V_iC_j)$ value base on the current $S_t$, and join $V_iC_j$ into $S_t$, until there is no more $V_iC_j$ that is worth selecting.</p>
<p>Below is the pseudo code of this greedy algorithm:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Initialize St as an empty set &#123;&#125;</span><br><span class="line">Initialize a WaitingList as a full set of video x cache &#123;ViCj | 0 ≤ i ≤ Nv, 0 ≤ j ≤ Nc&#125;</span><br><span class="line">Iteratively do: &#x2F;&#x2F; find the best VxCy</span><br><span class="line">    Calculate R(St, ViCj) for each ViCj ∈ WaitingList</span><br><span class="line">    If all R(St, ViCj) is not positive, then end the loop</span><br><span class="line">    Select VxCy with the maximum R(St, ViCj) value</span><br><span class="line">    Update St as St ∪ VxCy</span><br><span class="line">    Remove VxCy from WaitingList</span><br><span class="line">Return St as answer</span><br></pre></td></tr></table></figure>
<p>The concept of the algorithm itself is quite simple, but if there is no proper process, the time complexity is actually considerably large. That is because every time $R(S_t, V_iC_j)$ is calculated, all requests and all caches and all the videos, etc must be traversed. Although the time usage of the program does not affect the score, it is still annoying to have a slow program. So I dealt with the data structure especially, such as maintaining each request about which cache is currently the fastest to get the video from, and maintaining each video about which requests are affected, etc. In this way, the time needed to calculate a single $R(S_t, V_iC_j)$ can be greatly reduced.</p>
<h3 id="Efficiency-Improvement-Use-Priority-Queue"><a href="#Efficiency-Improvement-Use-Priority-Queue" class="headerlink" title="Efficiency Improvement - Use Priority Queue"></a>Efficiency Improvement - Use Priority Queue</h3><p>Another tricky problem is that “Recalculating each $R(S_t, V_iC_j)$ for each $V_iC_j$ in WaitingList” costs too much time. After all, there are up to ten thousand videos and one thousand caches. However, I only care about “Who’s $R(S_t, V_iC_j)$ is the maximum” in each iteration, so I didn’t actually calculate the reward of all the combinations. Instead, I used <strong>Priority queue</strong> to maintain the reward value corresponding to each $V_iC_j$</p>
<p>Priority Queue (PQ) is actually a Heap, which can let us get the maximum element in $O(logn)$ time. By the way, the coding language I used in my implementation is C++. </p>
<p>The reasons I chose Priority Queue to deal with this problem are below two:<br>1. I only need to know “who is the maximum” in each iteration<br>2. For any $V_iC_j$, after each iteration, its reward value $R(S_t, V_iC_j)$ always becomes less or stays the same. (This is essential!)</p>
<p>Because I used PQ instead of the original 2D array to store $R(S_t, V_iC_j)$, and only focus on PQ.top, so the most thing to worry about is that the value of $R(S_t, V_iC_j)$ would become outdated due to the update of $S_t$. However, because of the property of “reward decreasing”, I certainly can only check if PQ.top is up-to-date or not. If PQ.top is up-to-date, then even the other elements in PQ are outdated, I can still be sure that PQ.top is the maximum I want. In this way, I can considerably save the time to recalculate all $R(S_t, V_iC_j)$ in each iteration.</p>
<p>Below is the pseudo code of the greedy algorithm with priority queue:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Initialize St as an empty set &#123;&#125;</span><br><span class="line">Push all of the combinations of video x cache &#123;ViCj | 0 ≤ i ≤ Nv, 0 ≤ j ≤ Nc&#125; into PQ</span><br><span class="line">Iteratively do: &#x2F;&#x2F; Find the best VxCy</span><br><span class="line">    Iteratively do： &#x2F;&#x2F; Find the first PQ.top that is up-to-date</span><br><span class="line">        Take VxCy from PQ.top </span><br><span class="line">        Recalculate R(St, VxCy) based on the current St </span><br><span class="line">        If the new R(St, VxCy) is equal to PQ.top&#39;s value, then bring this VxCy and break the loop</span><br><span class="line">        Otherwise, push the new R(St, VxCy) into PQ and continue this loop</span><br><span class="line">    Update St as St ∪ VxCy</span><br><span class="line">Return St as the answer</span><br></pre></td></tr></table></figure>
<p>So far, the program has been able to run faster and better, but I have added some modifications to improve the score.</p>
<h3 id="Unit-Reward"><a href="#Unit-Reward" class="headerlink" title="Unit Reward"></a>Unit Reward</h3><p>My greedy strategy is to always greedily select $V_iC_j$ that maximizes the immediate reward, completely disregarding the possible bad situation. For example, choose the video too large that the cache server becomes full too quickly.</p>
<p><img src="https://i.imgur.com/pRHewLW.png"></p>
<p>Take this case as an example, my greedy strategy will choose $V_0$ to put into the cache server, then the cache server cannot accommodate any other videos. But choosing $V_1+V_2$ can get the higher score. To solve this issue, I modify the original immediate reward into “unit reward”, which is to divide the original reward by the size of the video.</p>
<p>$UR(S, V_xC_y)=\frac{R(S, V_xC_y)}{size(V_x)} = \frac{score(S \cup V_xC_y)-score(S)}{size(V_x)}$</p>
<p><img src="https://i.imgur.com/epGoi6L.png"></p>
<p>Note that even the reward function is modified, it is still a decreasing function, so the PQ technique still applies. After changing the reward function, the score really made a big leap forward.</p>
<h3 id="Randomness"><a href="#Randomness" class="headerlink" title="Randomness"></a>Randomness</h3><p>Finally is my struggle after I entered the top 5. LOL. At this moment there should be only a few hours before the end of the competition. My algorithm still has a fatal flaw: It is deterministic, there is no mechanism to explore the potential better solution. In order to break through this flow, I added the randomness into my algorithm.</p>
<p>In the greedy algorithm I mentioned before, I always select $V_xC_y$ with the maximum reward value. And with the randomness, under a certain probability, I’ll discard this $V_xC_y$ away. I believe that sometimes giving up on some seemingly good $V_xC_y$ can leave the cache space for some not-so-good $V_iC_j$ to use, and achieve a higher score.</p>
<p>It was almost the morning and I had to work the next day, so I modified the program to keep repeating the random algorithm. If it finds an answer with a higher score, it would overwrite the old answer. And then I went to sleep with the alarm clock to wake me up half an hour before the end of the competition to upload the latest answer. But in fact, this is really just a struggling. The score can only be improved a bit, there is no big leap forward. LOL</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>After the competition, I was invited to give a talk to share my solution because I was in the top 4. </p>
<p>After the competition, because  I was in the top four, I received an invitation to share the solution with everyone in the post-event. In addition to sharing the solution, the employees of the Hash Code Team also came to share what kind of test data they designed in this competition. I guess they should be looking forward to hearing what algorithms we have designed especially for the test cases they designed, but I think everyone seems to be just like me and only design a general algorithm. I think the main reason is that 24 hours is really too short, and analyzing data is not something that can be done casually and has good results (maybe I need a powerful teammate!?).</p>
<p>I am also very curious about what kind of team the Hash Code Team is. If I have the opportunity to participate in the design of Hash Code problems, it should be very interesting. I hope I will have the opportunity to check it out in the future. :)</p>

            </div>

            <!-- Post Comments -->
            

        </div>
        <!-- Copyright 版权 start -->
                <div id="copyright">
            <ul>
                <li>&copy;Powered By <a target="_blank" rel="noopener" href="https://hexo.io/zh-cn/" style="border-bottom: none;">hexo</a></li>
                <li>Design: <a target="_blank" rel="noopener" href="http://miccall.tech " style="border-bottom: none;">miccall</a></li>
            </ul>
            
				<span id="busuanzi_container_site_pv"> 2021 </span> 
			
        </div>
    </div>
</body>



 	
</html>
